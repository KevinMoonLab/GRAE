{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Extendable and invertible manifold learning with geometry regularized autoencoders (GRAE)\n",
    "\n",
    "The current version of this notebook aims to reproduce results for Autoencoders, GRAE and UMAP over 1 run only for demonstration purposes. All results can be\n",
    "reproduced over more runs by changing the \"Fit models/Experiment parameters\" subsection. Ideally, a GPU should be selected to run experiments (Runtime->Change runtime type->Hardware Accelerator->GPU in Google Colab).\n",
    "\n",
    "See the Topological Autoencoders subsection to learn how to set up this notebook\n",
    "to benchmark them. Otherwise, they are not available by default.\n",
    "\n",
    "We computed the Diffusion Nets embeddings using their source code, which is not reproduced here."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dependencies, seeds and utilities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Install unavailable dependencies\n",
    "# %%capture\n",
    "#\n",
    "# !pip install phate\n",
    "# !pip install scikit-bio"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 04 23:36:37 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 451.67       Driver Version: 451.67       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 206... WDDM  | 00000000:08:00.0  On |                  N/A |\n",
      "| 31%   31C    P8     9W / 184W |   1389MiB /  8192MiB |      9%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1196    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      2164    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      6116    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      6876    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      7484    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7884    C+G   ...w5n1h2txyewy\\SearchUI.exe    N/A      |\n",
      "|    0   N/A  N/A      8428    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8828    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A      8844    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n",
      "|    0   N/A  N/A     10892    C+G   ...slack\\app-4.7.0\\slack.exe    N/A      |\n",
      "|    0   N/A  N/A     11432    C+G   ...t\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     12080    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     12192    C+G   ...t\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     13100    C+G   ...qxf38zg5c\\Skype\\Skype.exe    N/A      |\n",
      "|    0   N/A  N/A     14664    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A     17184    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     17392    C+G   ...es.TextInput.InputApp.exe    N/A      |\n",
      "|    0   N/A  N/A     23224    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     29180    C+G   ...Battle.net\\Battle.net.exe    N/A      |\n",
      "|    0   N/A  N/A     29948    C+G   ...020.1.3\\bin\\pycharm64.exe    N/A      |\n",
      "|    0   N/A  N/A     31944    C+G   ...Battle.net\\Battle.net.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skbio.stats.distance import mantel\n",
    "\n",
    "# Datasets import\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as torch_datasets\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import ndimage\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "import urllib\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Model imports\n",
    "import umap\n",
    "import phate\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Experiment\n",
    "# Experiment parameters\n",
    "RUNS = 1\n",
    "FIT_RATIO = .8\n",
    "DATASETS = ['ribbons', 'Faces', 'RotatedDigits']\n",
    "MODELS = ['AE', 'GRAE_10', 'GRAE_100', 'Umap_t']\n",
    "\n",
    "# Model parameters\n",
    "BATCH = 200\n",
    "LR = .0001\n",
    "WEIGHT_DECAY = 1\n",
    "EPOCHS = 2\n",
    "\n",
    "# Dataset parameters\n",
    "FIT_DEFAULT = .8  # Ratio of data to use for training\n",
    "SAMPLE = 1000  # Number of points to sample from synthetic manifolds\n",
    "\n",
    "# Seeds and base path for data\n",
    "BASEPATH = 'data'\n",
    "SEED = 7512183 # Used for data generation and splits\n",
    "\n",
    "# 20 Random states for different training runs. Add more if you need more runs\n",
    "RANDOM_STATES = [36087, 63286, 52270, 10387, 40556, 52487, 26512, 28571, 33380,\n",
    "                9369, 28478,  4624, 29114, 41915,  6467,  4216, 16025, 34823,\n",
    "                29854, 23853]\n",
    "\n",
    "# Set seed for both torch and numpy\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Create directory for data\n",
    "if not os.path.exists(BASEPATH):\n",
    "  os.mkdir(BASEPATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:59: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:59: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:59: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:59: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-5-f0261afcd3aa>:59: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if m is 'reconstruction' or m.split('_')[0] is 'mrre':\n",
      "<ipython-input-5-f0261afcd3aa>:59: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if m is 'reconstruction' or m.split('_')[0] is 'mrre':\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "# Results Logger\n",
    "class Book():\n",
    "  def __init__(self, datasets, models, metrics):\n",
    "    self.col = ['model', 'dataset', 'run', 'split'] + metrics\n",
    "    self.log = list()\n",
    "    self.models = models\n",
    "    self.datasets = datasets\n",
    "    self.splits = ('train', 'test')\n",
    "    self.metrics = metrics\n",
    "\n",
    "  def add_entry(self, model, dataset, run, split, **kwargs):\n",
    "    # Proof read entry\n",
    "    self.check(model, dataset, split)\n",
    "    self.check_metrics(kwargs)\n",
    "\n",
    "    metrics_ordered = [kwargs[k] for k in self.metrics]\n",
    "\n",
    "    signature = [model, dataset, run, split]\n",
    "    entry = signature +  metrics_ordered\n",
    "\n",
    "    if len(entry) is not len(self.col):\n",
    "      raise Exception('Entry size is wrong.')\n",
    "\n",
    "    self.log.append(entry)\n",
    "\n",
    "  def check(self, model, dataset, split):\n",
    "    if model not in self.models:\n",
    "      raise Exception('Invalid model name.')\n",
    "\n",
    "    if dataset not in self.datasets:\n",
    "      raise Exception('Invalid dataset name.')\n",
    "\n",
    "    if split not in self.splits:\n",
    "      raise Exception('Invalid split name.')\n",
    "\n",
    "  def check_metrics(self, kwargs):\n",
    "    if len(kwargs.keys()) is not len(self.metrics):\n",
    "      raise Exception('Wrong number of metrics.')\n",
    "\n",
    "    for key in kwargs.keys():\n",
    "      if key not in self.metrics:\n",
    "        raise Exception(f'Trying to add undeclared metric {key}')\n",
    "\n",
    "\n",
    "  def get_df(self):\n",
    "    return pd.DataFrame.from_records(self.log, columns=self.col)\n",
    "\n",
    "\n",
    "def refine_df(df, df_metrics):\n",
    "  df_group = df.groupby(['split', 'dataset', 'model'])\n",
    "  mean = df_group.mean().drop(columns=['run']).round(4)\n",
    "\n",
    "  # Add rank columns\n",
    "  for m in df_metrics:\n",
    "    # Higher is better\n",
    "    ascending = False\n",
    "\n",
    "    if m is 'reconstruction' or m.split('_')[0] is 'mrre':\n",
    "      # Lower is better\n",
    "      ascending = True\n",
    "\n",
    "    loc = mean.columns.get_loc(m) + 1\n",
    "    rank = mean.groupby(['split', 'dataset'])[m].rank(method='min', ascending=ascending)\n",
    "    mean.insert(loc=loc, column=f'{m}_rank', value = rank)\n",
    "\n",
    "  return mean\n",
    "\n",
    "\n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "def slice_3D(X, Y, idx, axis, p=1):\n",
    "  axis = X[:, axis]\n",
    "\n",
    "  sli = np.zeros(shape=X.shape[0])\n",
    "  sli[idx] = 1\n",
    "\n",
    "  sampler = np.random.choice(a=[False, True], size=(sli.shape[0],), p=[1-p, p])\n",
    "\n",
    "  sli = np.logical_and(sli, sampler)\n",
    "\n",
    "  rest = np.logical_not(sli)\n",
    "\n",
    "  X_2, Y_2 = X[rest], Y[rest]\n",
    "  X_3, Y_3 = X[sli], Y[sli]\n",
    "\n",
    "\n",
    "  return X_2, Y_2, X_3, Y_3\n",
    "\n",
    "def make_holes(x, y, n=12, eps_range=(.2, .5), seed=SEED):\n",
    "  np.random.seed(SEED)\n",
    "\n",
    "  hole_idx = np.random.choice(x.shape[0], size=n, replace=False)\n",
    "  d = squareform(pdist(x))\n",
    "  eps_list = np.random.uniform(eps_range[0], eps_range[1], n)\n",
    "\n",
    "  test_idx = list()\n",
    "\n",
    "  for i, idx in enumerate(hole_idx):\n",
    "    d_line, eps = d[idx], eps_list[i]\n",
    "    test_idx.append(np.argwhere(d_line < eps))\n",
    "\n",
    "  test_idx = np.unique(np.concatenate(test_idx))\n",
    "  train_idx = np.full(fill_value=True, shape=x.shape[0])\n",
    "  train_idx[test_idx] = False\n",
    "\n",
    "  return x[train_idx], y[train_idx], x[test_idx], y[test_idx]\n",
    "\n",
    "\n",
    "def plot_3D(x, y, z, c):\n",
    "  fig, (a1)  = plt.subplots(1,1, figsize=(10, 10))\n",
    "  a1 = fig.add_subplot(111, projection='3d')\n",
    "  a1.scatter(x, y, z, c=c, cmap='jet', s = 2)\n",
    "  a1.view_init(elev=10, azim = 90)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def plot_3D_grey(x_train, y_train, z_train, x_test, y_test, z_test, c):\n",
    "  fig, (a1)  = plt.subplots(1,1, figsize=(10, 10))\n",
    "  a1 = fig.add_subplot(111, projection='3d')\n",
    "  a1.scatter(x_train, y_train, z_train, color='grey', s = 2, alpha=.5)\n",
    "  a1.scatter(x_test, y_test, z_test, c=c, cmap='jet', s = 2)\n",
    "  a1.view_init(elev=10, azim = 90)\n",
    "\n",
    "  plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:27: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:35: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:126: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:135: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:27: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:35: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:126: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:135: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-6-e3bedf470aeb>:27: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if split is 'none':\n",
      "<ipython-input-6-e3bedf470aeb>:35: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if split is 'train':\n",
      "<ipython-input-6-e3bedf470aeb>:126: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if split is 'none':\n",
      "<ipython-input-6-e3bedf470aeb>:135: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if split is 'train':\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "# Base class\n",
    "class BaseDataset(Dataset):\n",
    "  \"\"\"Simple class for X and Y ndarrays.\"\"\"\n",
    "  def __init__(self, x, y, split, split_ratio, seed):\n",
    "    if split not in ('train', 'test', 'none'):\n",
    "        raise Exception('split argument should be \"train\", \"test\" or \"none\"')\n",
    "\n",
    "    x, y = self.get_split(x, y, split, split_ratio, seed)\n",
    "\n",
    "    self.data = x.float()\n",
    "    self.targets = y.float()\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.data[index], self.targets[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def numpy(self, idx=None):\n",
    "    if idx is None:\n",
    "      return self.data.numpy(), self.targets.numpy()\n",
    "    else:\n",
    "      return self.data.numpy()[idx], self.targets.numpy()[idx]\n",
    "\n",
    "  def get_split(self, x, y, split, split_ratio, seed):\n",
    "      if split is 'none':\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "      n = x.shape[0]\n",
    "      train_idx, test_idx = train_test_split(np.arange(n),\n",
    "                                            train_size=split_ratio,\n",
    "                                            random_state=seed)\n",
    "\n",
    "      if split is 'train':\n",
    "        return torch.from_numpy(x[train_idx]), torch.from_numpy(y[train_idx])\n",
    "      else:\n",
    "        return torch.from_numpy(x[test_idx]), torch.from_numpy(y[test_idx])\n",
    "\n",
    "\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "  \"\"\"Wrapper for x ndarray with no targets.\"\"\"\n",
    "  def __init__(self, x):\n",
    "    self.data = torch.from_numpy(x).float()\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.data[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def numpy(self, idx=None):\n",
    "    if idx is None:\n",
    "      return self.data.numpy()\n",
    "    else:\n",
    "      return self.data.numpy()[idx]\n",
    "\n",
    "\n",
    "class SwissRoll(BaseDataset):\n",
    "    \"\"\"Standard Swiss Roll class.\"\"\"\n",
    "    def __init__(self, n_samples=SAMPLE, split='none', split_ratio=FIT_DEFAULT,\n",
    "                 seed=SEED, factor=6, sli_points=250):\n",
    "      x, y = datasets.make_swiss_roll(n_samples=n_samples, random_state=seed)\n",
    "\n",
    "      # Backup first axis, as it represents one of the underlying latent\n",
    "      # variable we aim to recover\n",
    "      self.y_pure = copy.deepcopy(x[:, 1])\n",
    "\n",
    "      # Normalize\n",
    "      x = scipy.stats.zscore(x)\n",
    "\n",
    "      # Get absolute distance from origin\n",
    "      ab = np.abs(x[:, 1])\n",
    "      sort = np.argsort(ab)\n",
    "\n",
    "      # Take the sli_points points closest to origin\n",
    "      # This is not used by the base class, but will be used by the Ribbons\n",
    "      # children class to remove a thin slice from the roll\n",
    "      self.test_idx = sort[0:sli_points]\n",
    "\n",
    "\n",
    "      # Apply rotation  to achieve same variance on all axes\n",
    "      x[:, 1] *= factor\n",
    "\n",
    "      theta = math.pi/4\n",
    "      phi = math.pi/3.3\n",
    "      rho = 0\n",
    "\n",
    "      cos = math.cos(theta)\n",
    "      sin = math.sin(theta)\n",
    "\n",
    "      cos_phi = math.cos(phi)\n",
    "      sin_phi = math.sin(phi)\n",
    "\n",
    "      cos_rho = math.cos(rho)\n",
    "      sin_rho = math.sin(rho)\n",
    "\n",
    "      rot = np.array([[cos, 0, sin],[0, 1, 0],[-sin, 0, cos]])\n",
    "\n",
    "      rot_2 = np.array([[cos_phi, -sin_phi, 0],[sin_phi, cos_phi, 0],[0, 0, 1]])\n",
    "      rot_3 = np.array([[1, 0, 0],[0, cos_rho, -sin_rho],[0, sin_rho, cos_rho]])\n",
    "\n",
    "\n",
    "      x = x  @ rot_2 @ rot_3 @ rot\n",
    "      x = scipy.stats.zscore(x) # Normalize for true unit variance\n",
    "\n",
    "      super().__init__(x, y, split, split_ratio, seed)\n",
    "\n",
    "    def get_source(self):\n",
    "      # First source is coloring (representing the 'rolled' axis), second is\n",
    "      # the 'untransformed' first axis (length of the roll)\n",
    "      return self.targets.numpy(), self.y_pure\n",
    "\n",
    "\n",
    "class Ribbons(SwissRoll):\n",
    "    \"\"\"Swiss Roll class where the test split is a thin 'ribbon' of sli_points\n",
    "    points removed from the middle of the manifold.\"\"\"\n",
    "    def __init__(self, n_samples=SAMPLE, sli_points=250, split='none',\n",
    "                 split_ratio=FIT_DEFAULT, seed=SEED):\n",
    "\n",
    "      super().__init__(n_samples, split, split_ratio=split_ratio, seed=seed,\n",
    "                       factor=6, sli_points=sli_points)\n",
    "\n",
    "    def get_split(self, x, y, split, split_ratio, seed):\n",
    "      if split is 'none':\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "      x_train, y_train, x_test, y_test = slice_3D(x, y, self.test_idx, 1)\n",
    "      train_mask = np.full(x.shape[0], fill_value=True)\n",
    "      train_mask[self.test_idx] = False\n",
    "      test_mask = np.logical_not(train_mask)\n",
    "\n",
    "\n",
    "      if split is 'train':\n",
    "        self.y_pure = self.y_pure[train_mask]\n",
    "        return torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
    "      else:\n",
    "        self.y_pure = self.y_pure[test_mask]\n",
    "        return torch.from_numpy(x_test), torch.from_numpy(y_test)\n",
    "\n",
    "    def get_source(self):\n",
    "      # First source is coloring, second is the axis 1 (length of the roll)\n",
    "      return self.targets.numpy(), self.y_pure\n",
    "\n",
    "\n",
    "class Rotated(BaseDataset):\n",
    "    def __init__(self, fetcher, split='none', split_ratio=FIT_DEFAULT,\n",
    "                 seed=SEED, n_images=3, n_rotations=360, max_degree=360):\n",
    "      \"\"\"Pick n_images of n different classes and return a dataset with\n",
    "         n_rotations for each image.\"\"\"\n",
    "      self.max_degree = max_degree\n",
    "\n",
    "      np.random.seed(seed)\n",
    "\n",
    "      transforms_MNIST = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "      ])\n",
    "\n",
    "      train = fetcher(root=BASEPATH, train=True, download=True,\n",
    "                      transform=transforms_MNIST)\n",
    "\n",
    "      X = train.data.detach().numpy().reshape(60000,784)\n",
    "      X = X/X.max()\n",
    "      Y = train.targets.detach().numpy()\n",
    "\n",
    "      # Pick classes\n",
    "      classes = np.random.choice(10, size=n_images, replace=False)\n",
    "\n",
    "      imgs = list()\n",
    "\n",
    "      for i in classes:\n",
    "        subset = X[Y == i]\n",
    "        i = np.random.choice(subset.shape[0], size=1)\n",
    "        imgs.append(subset[i])\n",
    "\n",
    "\n",
    "      def generate_rotations(img, c, N):\n",
    "        img = img.reshape((28,28))\n",
    "\n",
    "        new_angles = np.linspace(0, self.max_degree, num=N, endpoint=False)\n",
    "\n",
    "        img_new = np.zeros((N,28,28))\n",
    "\n",
    "        for i, ang in enumerate(new_angles):\n",
    "          img_new[i,:,:] = ndimage.rotate(img, ang, reshape=False)\n",
    "\n",
    "\n",
    "          X1 = img_new.reshape(len(new_angles), 784)\n",
    "\n",
    "        return X1, np.full(shape=(N,), fill_value=c)\n",
    "\n",
    "\n",
    "      rotations = [generate_rotations(img, c=i, N=n_rotations)\n",
    "        for i, img in enumerate(imgs)]\n",
    "\n",
    "      X_rotated, Y_rotated = zip(*rotations)\n",
    "\n",
    "      X_rotated  = np.concatenate(X_rotated)\n",
    "      Y_rotated  = np.concatenate(Y_rotated)\n",
    "\n",
    "\n",
    "      super().__init__(X_rotated, Y_rotated, split, split_ratio, seed)\n",
    "\n",
    "\n",
    "class RotatedDigits(Rotated):\n",
    "  \"\"\"3 rotated MNIST digits with 360 rotations per image, for a total of\n",
    "  1080 samples.\"\"\"\n",
    "  def __init__(self, split='none', split_ratio=FIT_DEFAULT, seed=SEED,\n",
    "               n_images=3, n_rotations=360):\n",
    "    super().__init__(torch_datasets.MNIST, split, split_ratio, seed,\n",
    "                     n_images, n_rotations)\n",
    "\n",
    "\n",
    "\n",
    "class Faces(BaseDataset):\n",
    "    def __init__(self, split='none', split_ratio=FIT_DEFAULT, seed=SEED):\n",
    "\n",
    "      self.url = 'http://stt3795.guywolf.org/Devoirs/D02/face_data.mat'\n",
    "      self.root = os.path.join(BASEPATH, 'faces')\n",
    "\n",
    "      if not os.path.exists(self.root):\n",
    "        os.mkdir(self.root)\n",
    "        self._download()\n",
    "\n",
    "      d = loadmat(os.path.join(self.root, 'face_data.mat'))\n",
    "\n",
    "      x = d['images'].T\n",
    "\n",
    "      y = d['poses'].T\n",
    "\n",
    "      super().__init__(x, y, split, split_ratio, seed)\n",
    "\n",
    "\n",
    "      # Save y_1 and y_2 for coloring\n",
    "      self.y_2 = self.targets[:, 1].numpy()\n",
    "      self.y_1 = self.targets[:, 0].numpy()\n",
    "\n",
    "      # Keep only one latent in the targets attribute for compatibility with\n",
    "      # other datasets\n",
    "      self.targets = self.targets[:, 0]\n",
    "\n",
    "    def _download(self):\n",
    "      print('Downloading Faces dataset')\n",
    "      urllib.request.urlretrieve(self.url, os.path.join(self.root, 'face_data.mat'))\n",
    "\n",
    "    def get_source(self):\n",
    "      return self.y_1, self.y_2\n",
    "\n",
    "\n",
    "# Dict to call dataset constructors using a string\n",
    "dataset_constructors = dict(\n",
    "    ribbons=Ribbons,\n",
    "    Faces=Faces,\n",
    "    RotatedDigits=RotatedDigits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AE Definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Vanilla AE\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)\n",
    "\n",
    "\n",
    "\n",
    "# AE building blocks\n",
    "class Encoder_MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.linear3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.mu = nn.Linear(hidden_dim3, z_dim)\n",
    "    def forward(self, x):\n",
    "        hidden1 = F.relu(self.linear(x))\n",
    "        hidden2 = F.relu(self.linear2(hidden1))\n",
    "        hidden3 = F.relu(self.linear3(hidden2))\n",
    "        z_mu = self.mu(hidden3)\n",
    "        return z_mu\n",
    "\n",
    "class Decoder_MLP(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim, sigmoid_act = False):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.linear3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.out = nn.Linear(hidden_dim3, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid_act = sigmoid_act\n",
    "    def forward(self, x):\n",
    "        hidden1 = F.relu(self.linear(x))\n",
    "        hidden2 = F.relu(self.linear2(hidden1))\n",
    "        hidden3 = F.relu(self.linear3(hidden2))\n",
    "        if self.sigmoid_act == False:\n",
    "            predicted = (self.out(hidden3))\n",
    "        else:\n",
    "            predicted = F.sigmoid(self.out(hidden3))\n",
    "        return predicted\n",
    "\n",
    "class AE_MLP(nn.Module):\n",
    "    def __init__(self, enc, dec):\n",
    "        super().__init__()\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.enc(x)\n",
    "        predicted = self.dec(latent)\n",
    "        return predicted, latent\n",
    "\n",
    "\n",
    "# AE main class\n",
    "class AE():\n",
    "    \"\"\"Autoencoder class with sklearn interface.\"\"\"\n",
    "    def __init__(self, input_size, random_state=SEED, track_rec=False,\n",
    "                 AE_wrapper=AE_MLP, batch_size=BATCH, lr=LR,\n",
    "                 weight_decay=WEIGHT_DECAY, reduction='sum', epochs=EPOCHS, **kwargs):\n",
    "      layer_1 = 800\n",
    "      layer_2 = 400\n",
    "      layer_3 = 200\n",
    "      self.lr = lr\n",
    "      self.epochs = epochs\n",
    "      self.batch_size = batch_size\n",
    "      self.weight_decay = weight_decay\n",
    "      self.encoder = Encoder_MLP(input_size, layer_1, layer_2, layer_3, 2)\n",
    "      self.decoder = Decoder_MLP(2, layer_3, layer_2, layer_1, input_size)\n",
    "      self.model = AE_wrapper(self.encoder, self.decoder, **kwargs)\n",
    "      self.model = self.model.float().to(device)\n",
    "\n",
    "      self.criterion = nn.MSELoss(reduction=reduction)\n",
    "\n",
    "      self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                        lr = self.lr,\n",
    "                                        weight_decay=self.weight_decay)\n",
    "      self.loss = list()\n",
    "      self.track_rec = track_rec\n",
    "      self.random_state = random_state\n",
    "\n",
    "    def fit(self, x):\n",
    "      # Train AE\n",
    "      self.model.train()\n",
    "\n",
    "      # Reproducibility\n",
    "      torch.manual_seed(self.random_state)\n",
    "      torch.backends.cudnn.deterministic = True\n",
    "      torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "      loader = torch.utils.data.DataLoader(x, batch_size=self.batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "      for epoch in range(self.epochs):\n",
    "        for batch in loader:\n",
    "            data, y = batch\n",
    "            data = data.to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            x_hat, _ = self.model(data)\n",
    "            x_hat = x_hat.to(device)\n",
    "            loss = self.criterion(data, x_hat)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.track_rec:\n",
    "          x_np, _ = x.numpy()\n",
    "          x_hat = self.inverse_transform(self.transform(x))\n",
    "          self.loss.append(mean_squared_error(x_np, x_hat))\n",
    "\n",
    "    def transform(self, x):\n",
    "      self.model.eval()\n",
    "      loader = torch.utils.data.DataLoader(x, batch_size=self.batch_size,\n",
    "                                           shuffle=False)\n",
    "      z = [self.encoder(batch.to(device)).cpu().detach().numpy()\n",
    "      for batch, _ in loader]\n",
    "\n",
    "      return np.concatenate(z)\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "      self.fit(x)\n",
    "      return self.transform(x)\n",
    "\n",
    "    def inverse_transform(self, z):\n",
    "      self.model.eval()\n",
    "      z = NumpyDataset(z)\n",
    "      loader = torch.utils.data.DataLoader(z, batch_size=self.batch_size,\n",
    "                                           shuffle=False)\n",
    "      x_hat = [self.decoder(batch.to(device)).cpu().detach().numpy()\n",
    "      for batch in loader]\n",
    "\n",
    "      return np.concatenate(x_hat)\n",
    "\n",
    "\n",
    "class ManifoldLoss(nn.Module):\n",
    "    def __init__(self, lam):\n",
    "      super().__init__()\n",
    "      self.lam = lam\n",
    "      self.MSE = nn.MSELoss(reduction='sum')\n",
    "      self.loss = None\n",
    "\n",
    "    def forward(self, x, y, z, emb):\n",
    "      self.loss = self.MSE(x, y) + self.lam * self.MSE(z, emb)\n",
    "      return self.loss\n",
    "\n",
    "    def backward(self):\n",
    "      self.loss.backward()\n",
    "\n",
    "    def decay_lam(self, factor):\n",
    "      self.lam *= factor\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GRAE Definitions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "class ManifoldNet(AE):\n",
    "  \"\"\"Base class for GRAE.\"\"\"\n",
    "  def __init__(self, input_size, embedder, random_state, track_rec=False, lam=10, lam_decay=1, **kwargs):\n",
    "      super().__init__(input_size, random_state, track_rec)\n",
    "      self.criterion = ManifoldLoss(lam)\n",
    "      self.emb = None\n",
    "      self.targets = None\n",
    "      self.precomputed = False\n",
    "      self.embedder_args = kwargs\n",
    "      self.embedder = embedder\n",
    "      self.lam_decay = lam_decay\n",
    "\n",
    "  def fit(self, x):\n",
    "    self.model.train()\n",
    "\n",
    "    # Reproducibility\n",
    "    torch.manual_seed(self.random_state)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    x_np, self.targets = x.numpy()\n",
    "\n",
    "    # Compute embedding target\n",
    "    if not self.precomputed:\n",
    "      embedder_m = self.embedder(**self.embedder_args,\n",
    "                                 random_state=self.random_state)\n",
    "\n",
    "      emb = embedder_m.fit_transform(x_np)\n",
    "\n",
    "      # Normalize\n",
    "      emb = scipy.stats.zscore(emb)\n",
    "\n",
    "      self.emb = emb\n",
    "\n",
    "\n",
    "    # Loader to iterate over both data batches and target embedding batches\n",
    "    data = ConcatDataset(torch.from_numpy(self.emb).float(), x)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(data,\n",
    "                                          batch_size=self.batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "    for epoch in range(self.epochs):\n",
    "      for embedding, batch in loader:\n",
    "          x_in, _ = batch[0].to(device), batch[1]\n",
    "          embedding = embedding.to(device)\n",
    "          self.optimizer.zero_grad()\n",
    "          x_hat, z_mu = self.model(x_in)\n",
    "\n",
    "          x_hat = x_hat.to(device)\n",
    "          z_mu = z_mu.to(device)\n",
    "          self.criterion(x_in, x_hat, z_mu, embedding)\n",
    "          self.criterion.backward()\n",
    "          self.optimizer.step()\n",
    "\n",
    "      self.criterion.decay_lam(self.lam_decay)\n",
    "\n",
    "      if self.track_rec:\n",
    "        x_hat = self.inverse_transform(self.transform(x))\n",
    "        self.loss.append(mean_squared_error(x_np, x_hat))\n",
    "\n",
    "  def plot_latent(self):\n",
    "    plt.scatter(*self.emb.T, c=self.targets, cmap=\"jet\")\n",
    "    plt.show()\n",
    "\n",
    "  def set_embedding(self, emb):\n",
    "    self.emb = emb\n",
    "    self.precomputed = True\n",
    "\n",
    "\n",
    "\n",
    "# Variants of GRAE\n",
    "class GRAE(ManifoldNet):\n",
    "  \"\"\"Vanilla GRAE.\"\"\"\n",
    "  def __init__(self, input_size, random_state=SEED, track_rec=False,\n",
    "               lam=10, lam_decay=1, t='auto', knn=20, n_landmark=2000,\n",
    "               mds='metric'):\n",
    "    super().__init__(input_size=input_size,\n",
    "                      random_state=random_state,\n",
    "                      embedder=phate.PHATE,\n",
    "                      track_rec=track_rec,\n",
    "                      lam=lam,\n",
    "                      lam_decay=lam_decay,\n",
    "                      t=t,\n",
    "                      knn=knn,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=0,\n",
    "                      n_landmark=n_landmark,\n",
    "                      mds=mds)\n",
    "\n",
    "\n",
    "class GRAE_UMAP(ManifoldNet):\n",
    "  \"\"\"UMAP GRAE, as presented in the supplement.\"\"\"\n",
    "  def __init__(self, input_size, random_state=SEED,\n",
    "               track_rec=False, lam=10, lam_decay=1, n_neighbors=15):\n",
    "    super().__init__(input_size=input_size,\n",
    "                      random_state=random_state,\n",
    "                      embedder=umap.UMAP,\n",
    "                      track_rec=track_rec,\n",
    "                      lam=lam,\n",
    "                      n_neighbors=n_neighbors)\n",
    "\n",
    "\n",
    "class GRAE_TSNE(ManifoldNet):\n",
    "  \"\"\"t-SNE GRAE, as presented in the supplement.\"\"\"\n",
    "  def __init__(self, input_size, random_state=SEED, track_rec=False,\n",
    "               lam=10, lam_decay=1, perplexity=30):\n",
    "    super().__init__(input_size=input_size,\n",
    "                      random_state=random_state,\n",
    "                      embedder=TSNE,\n",
    "                      track_rec=track_rec,\n",
    "                      lam=lam,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=0,\n",
    "                      perplexity=perplexity)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Topological Autoencoders\n",
    "If reproduction of the TopoAE results is needed, the original code of the authors (https://osf.io/abuce/?view_only=f16d65d3f73e4918ad07cdd08a1a0d4b)\n",
    "should be downloaded and saved under the folder topologically-constrained-autoencoder\n",
    "on Google Drive and the cells in this section should be uncommented.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# # Mount Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# sys.path.append('/content/drive/My Drive/topologically-constrained-autoencoder')\n",
    "# !pip install pipenv\n",
    "\n",
    "# !cd '/content/drive/My Drive/topologically-constrained-autoencoder'\n",
    "# !pipenv install\n",
    "\n",
    "# import src.models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# # TopoAE Wrapper\n",
    "\n",
    "# class TopoBaseAE(nn.Module):\n",
    "#   # Using TopoRegEdgeSymmetric from their results\n",
    "#   def __init__(self, enc, dec, lam):\n",
    "#       super().__init__()\n",
    "#       self.encoder = enc\n",
    "#       self.decoder = dec\n",
    "#       self.topo_sig = src.models.approx_based.TopologicalSignatureDistance(match_edges='symmetric')\n",
    "#       self.reconstruction = nn.MSELoss(reduction='sum')\n",
    "#       self.lam = lam\n",
    "#       self.latent_norm = torch.nn.Parameter(data=torch.ones(1),\n",
    "#                                               requires_grad=True)\n",
    "\n",
    "#   @staticmethod\n",
    "#   def _compute_distance_matrix(x, p=2):\n",
    "#       x_flat = x.view(x.size(0), -1)\n",
    "#       distances = torch.norm(x_flat[:, None] - x_flat, dim=2, p=p)\n",
    "#       return distances\n",
    "\n",
    "#   def forward(self, x):\n",
    "#       \"\"\"Compute the loss of the Topologically regularized autoencoder.\n",
    "\n",
    "#       Args:\n",
    "#           x: Input data\n",
    "\n",
    "#       Returns:\n",
    "#           Tuple of final_loss, (...loss components...)\n",
    "\n",
    "#       \"\"\"\n",
    "#       latent = self.encoder(x)\n",
    "#       x_rec = self.decoder(latent)\n",
    "\n",
    "#       x_distances = self._compute_distance_matrix(x)\n",
    "\n",
    "#       dimensions = x.size()\n",
    "#       if len(dimensions) == 4:\n",
    "#           # If we have an image dataset, normalize using theoretical maximum\n",
    "#           batch_size, ch, b, w = dimensions\n",
    "#           # Compute the maximum distance we could get in the data space (this\n",
    "#           # is only valid for images wich are normalized between -1 and 1)\n",
    "#           max_distance = (2**2 * ch * b * w) ** 0.5\n",
    "#           x_distances = x_distances / max_distance\n",
    "#       else:\n",
    "#           # Else just take the max distance we got in the batch\n",
    "#           x_distances = x_distances / x_distances.max()\n",
    "\n",
    "#       latent_distances = self._compute_distance_matrix(latent)\n",
    "#       latent_distances = latent_distances / self.latent_norm\n",
    "\n",
    "#       # Use reconstruction loss of autoencoder\n",
    "#       ae_loss = self.reconstruction(x, x_rec)\n",
    "\n",
    "\n",
    "\n",
    "#       topo_error, topo_error_components = self.topo_sig(\n",
    "#           x_distances, latent_distances)\n",
    "\n",
    "#       # normalize topo_error according to batch_size\n",
    "#       batch_size = dimensions[0]\n",
    "#       topo_error = topo_error / float(batch_size)\n",
    "#       loss = ae_loss + self.lam * topo_error\n",
    "\n",
    "#       return loss\n",
    "\n",
    "\n",
    "# class TopoAE(AE):\n",
    "#   def __init__(self, lam, input_size, random_state=SEED, track_rec=False, batch_size=BATCH, lr=LR, weight_decay=WEIGHT_DECAY):\n",
    "#     super().__init__(input_size=input_size,\n",
    "#                    random_state=random_state,\n",
    "#                    track_rec=track_rec,\n",
    "#                    AE_wrapper=TopoBaseAE,\n",
    "#                    batch_size = batch_size,\n",
    "#                    lr = lr,\n",
    "#                    lam=lam,\n",
    "#                    weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "#   def fit(self, x):\n",
    "#     # Train AE\n",
    "#     self.model.train()\n",
    "\n",
    "#     # Reproducibility\n",
    "#     torch.manual_seed(self.random_state)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#     x_np, _ = x.numpy()\n",
    "\n",
    "#     loader = torch.utils.data.DataLoader(x,batch_size=self.batch_size,\n",
    "#                                           shuffle=True)\n",
    "\n",
    "#     for epoch in range(self.epochs):\n",
    "#       for batch in loader:\n",
    "#           data, y = batch\n",
    "#           data = data.to(device)\n",
    "\n",
    "#           # TopoAE handles loss internally\n",
    "#           loss = self.model(data)\n",
    "\n",
    "#           self.optimizer.zero_grad()\n",
    "#           loss.backward()\n",
    "#           self.optimizer.step()\n",
    "\n",
    "#       if self.track_rec:\n",
    "#         x_hat = self.inverse_transform(self.transform(x))\n",
    "#         self.loss.append(mean_squared_error(x_np, x_hat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Unsupervised metrics (Table 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:288: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:291: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:288: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:291: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-11-2de1dd0687cb>:288: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if len(args) is 2:\n",
      "<ipython-input-11-2de1dd0687cb>:291: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif len(args) is 1:\n"
     ]
    }
   ],
   "source": [
    "# Unsupervised Metrics\n",
    "# Code adapted from the Topological autoencoders paper\n",
    "class MeasureCalculator():\n",
    "    # measures = MeasureRegistrator()\n",
    "\n",
    "    def __init__(self, X, Z, X_hat, k_max=20):\n",
    "        self.k_max = k_max\n",
    "        self.X = X\n",
    "        self.X_hat = X_hat\n",
    "        self.pairwise_X = squareform(pdist(X))\n",
    "        self.pairwise_Z = squareform(pdist(Z))\n",
    "        self.neighbours_X, self.ranks_X = \\\n",
    "            self._neighbours_and_ranks(self.pairwise_X, k_max)\n",
    "        self.neighbours_Z, self.ranks_Z = \\\n",
    "            self._neighbours_and_ranks(self.pairwise_Z, k_max)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _neighbours_and_ranks(distances, k):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - distances,        distance matrix [n times n],\n",
    "        - k,                number of nearest neighbours to consider\n",
    "        Returns:\n",
    "        - neighbourhood,    contains the sample indices (from 0 to n-1) of kth nearest neighbor of current sample [n times k]\n",
    "        - ranks,            contains the rank of each sample to each sample [n times n], whereas entry (i,j) gives the rank that sample j has to i (the how many 'closest' neighbour j is to i)\n",
    "        \"\"\"\n",
    "        # Warning: this is only the ordering of neighbours that we need to\n",
    "        # extract neighbourhoods below. The ranking comes later!\n",
    "        indices = np.argsort(distances, axis=-1, kind='stable')\n",
    "\n",
    "        # Extract neighbourhoods.\n",
    "        neighbourhood = indices[:, 1:k+1]\n",
    "\n",
    "        # Convert this into ranks (finally)\n",
    "        ranks = indices.argsort(axis=-1, kind='stable')\n",
    "\n",
    "        return neighbourhood, ranks\n",
    "\n",
    "    def get_X_neighbours_and_ranks(self, k):\n",
    "        return self.neighbours_X[:, :k], self.ranks_X\n",
    "\n",
    "    def get_Z_neighbours_and_ranks(self, k):\n",
    "        return self.neighbours_Z[:, :k], self.ranks_Z\n",
    "\n",
    "    def compute_k_independent_measures(self):\n",
    "        return {key: fn(self) for key, fn in\n",
    "                self.measures.get_k_independent_measures().items()}\n",
    "\n",
    "    def compute_k_dependent_measures(self, k):\n",
    "        return {key: fn(self, k) for key, fn in\n",
    "                self.measures.get_k_dependent_measures().items()}\n",
    "\n",
    "    def compute_measures_for_ks(self, ks):\n",
    "        return {\n",
    "            key: np.array([fn(self, k) for k in ks])\n",
    "            for key, fn in self.measures.get_k_dependent_measures().items()\n",
    "        }\n",
    "\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def stress(self):\n",
    "        sum_of_squared_differences = \\\n",
    "            np.square(self.pairwise_X - self.pairwise_Z).sum()\n",
    "        sum_of_squares = np.square(self.pairwise_Z).sum()\n",
    "\n",
    "        return np.sqrt(sum_of_squared_differences / sum_of_squares)\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def rmse(self):\n",
    "        n = self.pairwise_X.shape[0]\n",
    "        sum_of_squared_differences = np.square(\n",
    "            self.pairwise_X - self.pairwise_Z).sum()\n",
    "        return np.sqrt(sum_of_squared_differences / n**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _trustworthiness(X_neighbourhood, X_ranks, Z_neighbourhood,\n",
    "                         Z_ranks, n, k):\n",
    "        '''\n",
    "        Calculates the trustworthiness measure between the data space `X`\n",
    "        and the latent space `Z`, given a neighbourhood parameter `k` for\n",
    "        defining the extent of neighbourhoods.\n",
    "        '''\n",
    "\n",
    "        result = 0.0\n",
    "\n",
    "        # Calculate number of neighbours that are in the $k$-neighbourhood\n",
    "        # of the latent space but not in the $k$-neighbourhood of the data\n",
    "        # space.\n",
    "        for row in range(X_ranks.shape[0]):\n",
    "            missing_neighbours = np.setdiff1d(\n",
    "                Z_neighbourhood[row],\n",
    "                X_neighbourhood[row]\n",
    "            )\n",
    "\n",
    "            for neighbour in missing_neighbours:\n",
    "                result += (X_ranks[row, neighbour] - k)\n",
    "\n",
    "        return 1 - 2 / (n * k * (2 * n - 3 * k - 1) ) * result\n",
    "\n",
    "    # @measures.register(True)\n",
    "    def trustworthiness(self, k):\n",
    "        X_neighbourhood, X_ranks = self.get_X_neighbours_and_ranks(k)\n",
    "        Z_neighbourhood, Z_ranks = self.get_Z_neighbours_and_ranks(k)\n",
    "        n = self.pairwise_X.shape[0]\n",
    "        return self._trustworthiness(X_neighbourhood, X_ranks, Z_neighbourhood,\n",
    "                                     Z_ranks, n, k)\n",
    "\n",
    "    # @measures.register(True)\n",
    "    def continuity(self, k):\n",
    "        '''\n",
    "        Calculates the continuity measure between the data space `X` and the\n",
    "        latent space `Z`, given a neighbourhood parameter `k` for setting up\n",
    "        the extent of neighbourhoods.\n",
    "\n",
    "        This is just the 'flipped' variant of the 'trustworthiness' measure.\n",
    "        '''\n",
    "\n",
    "        X_neighbourhood, X_ranks = self.get_X_neighbours_and_ranks(k)\n",
    "        Z_neighbourhood, Z_ranks = self.get_Z_neighbours_and_ranks(k)\n",
    "        n = self.pairwise_X.shape[0]\n",
    "        # Notice that the parameters have to be flipped here.\n",
    "        return self._trustworthiness(Z_neighbourhood, Z_ranks, X_neighbourhood,\n",
    "                                     X_ranks, n, k)\n",
    "\n",
    "    # @measures.register(True)\n",
    "    def neighbourhood_loss(self, k):\n",
    "        '''\n",
    "        Calculates the neighbourhood loss quality measure between the data\n",
    "        space `X` and the latent space `Z` for some neighbourhood size $k$\n",
    "        that has to be pre-defined.\n",
    "        '''\n",
    "\n",
    "        X_neighbourhood, _ = self.get_X_neighbours_and_ranks(k)\n",
    "        Z_neighbourhood, _ = self.get_Z_neighbours_and_ranks(k)\n",
    "\n",
    "        result = 0.0\n",
    "        n = self.pairwise_X.shape[0]\n",
    "\n",
    "        for row in range(n):\n",
    "            shared_neighbours = np.intersect1d(\n",
    "                X_neighbourhood[row],\n",
    "                Z_neighbourhood[row],\n",
    "                assume_unique=True\n",
    "            )\n",
    "\n",
    "            result += len(shared_neighbours) / k\n",
    "\n",
    "        return 1.0 - result / n\n",
    "\n",
    "\n",
    "    # @measures.register(True)\n",
    "    def rank_correlation(self, k):\n",
    "        '''\n",
    "        Calculates the spearman rank correlation of the data\n",
    "        space `X` with respect to the latent space `Z`, subject to its $k$\n",
    "        nearest neighbours.\n",
    "        '''\n",
    "\n",
    "        X_neighbourhood, X_ranks = self.get_X_neighbours_and_ranks(k)\n",
    "        Z_neighbourhood, Z_ranks = self.get_Z_neighbours_and_ranks(k)\n",
    "\n",
    "        n = self.pairwise_X.shape[0]\n",
    "        #we gather\n",
    "        gathered_ranks_x = []\n",
    "        gathered_ranks_z = []\n",
    "        for row in range(n):\n",
    "            #we go from X to Z here:\n",
    "            for neighbour in X_neighbourhood[row]:\n",
    "                rx = X_ranks[row, neighbour]\n",
    "                rz = Z_ranks[row, neighbour]\n",
    "                gathered_ranks_x.append(rx)\n",
    "                gathered_ranks_z.append(rz)\n",
    "        rs_x = np.array(gathered_ranks_x)\n",
    "        rs_z = np.array(gathered_ranks_z)\n",
    "        coeff, _ = spearmanr(rs_x, rs_z)\n",
    "\n",
    "        ##use only off-diagonal (non-trivial) ranks:\n",
    "        #inds = ~np.eye(X_ranks.shape[0],dtype=bool)\n",
    "        #coeff, pval = spearmanr(X_ranks[inds], Z_ranks[inds])\n",
    "        return coeff\n",
    "\n",
    "    # @measures.register(True)\n",
    "    def mrre(self, k):\n",
    "        '''\n",
    "        Calculates the mean relative rank error quality metric of the data\n",
    "        space `X` with respect to the latent space `Z`, subject to its $k$\n",
    "        nearest neighbours.\n",
    "        '''\n",
    "\n",
    "        X_neighbourhood, X_ranks = self.get_X_neighbours_and_ranks(k)\n",
    "        Z_neighbourhood, Z_ranks = self.get_Z_neighbours_and_ranks(k)\n",
    "\n",
    "        n = self.pairwise_X.shape[0]\n",
    "\n",
    "        # First component goes from the latent space to the data space, i.e.\n",
    "        # the relative quality of neighbours in `Z`.\n",
    "\n",
    "        mrre_ZX = 0.0\n",
    "        for row in range(n):\n",
    "            for neighbour in Z_neighbourhood[row]:\n",
    "                rx = X_ranks[row, neighbour]\n",
    "                rz = Z_ranks[row, neighbour]\n",
    "\n",
    "                mrre_ZX += abs(rx - rz) / rz\n",
    "\n",
    "        # Second component goes from the data space to the latent space,\n",
    "        # i.e. the relative quality of neighbours in `X`.\n",
    "\n",
    "        mrre_XZ = 0.0\n",
    "        for row in range(n):\n",
    "            # Note that this uses a different neighbourhood definition!\n",
    "            for neighbour in X_neighbourhood[row]:\n",
    "                rx = X_ranks[row, neighbour]\n",
    "                rz = Z_ranks[row, neighbour]\n",
    "\n",
    "                # Note that this uses a different normalisation factor\n",
    "                mrre_XZ += abs(rx - rz) / rx\n",
    "\n",
    "        # Normalisation constant\n",
    "        C = n * sum([abs(2*j - n - 1) / j for j in range(1, k+1)])\n",
    "        # return mrre_ZX / C, mrre_XZ / C\n",
    "        return mrre_ZX / C\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_global(self, sigma=0.1):\n",
    "        X = self.pairwise_X\n",
    "        X = X / X.max()\n",
    "        Z = self.pairwise_Z\n",
    "        Z = Z / Z.max()\n",
    "\n",
    "        density_x = np.sum(np.exp(-(X ** 2) / sigma), axis=-1)\n",
    "        density_x /= density_x.sum(axis=-1)\n",
    "\n",
    "        density_z = np.sum(np.exp(-(Z ** 2) / sigma), axis=-1)\n",
    "        density_z /= density_z.sum(axis=-1)\n",
    "\n",
    "        return np.abs(density_x - density_z).sum()\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_kl_global(self, sigma=0.1):\n",
    "        X = self.pairwise_X\n",
    "        X = X / X.max()\n",
    "        Z = self.pairwise_Z\n",
    "        Z = Z / Z.max()\n",
    "\n",
    "        density_x = np.sum(np.exp(-(X ** 2) / sigma), axis=-1)\n",
    "        density_x /= density_x.sum(axis=-1)\n",
    "\n",
    "        density_z = np.sum(np.exp(-(Z ** 2) / sigma), axis=-1)\n",
    "        density_z /= density_z.sum(axis=-1)\n",
    "\n",
    "        return (density_x * (np.log(density_x) - np.log(density_z))).sum()\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_kl_global_10(self):\n",
    "        return self.density_kl_global(10.)\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_kl_global_1(self):\n",
    "        return self.density_kl_global(1.)\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_kl_global_01(self):\n",
    "        return self.density_kl_global(0.1)\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_kl_global_001(self):\n",
    "        return self.density_kl_global(0.01)\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_kl_global_0001(self):\n",
    "        return self.density_kl_global(0.001)\n",
    "\n",
    "    def reconstruction(self):\n",
    "      if self.X_hat is None:\n",
    "        return None\n",
    "\n",
    "      return mean_squared_error(self.X, self.X_hat)\n",
    "\n",
    "\n",
    "    def get_metrics(self, metrics):\n",
    "      results = dict()\n",
    "\n",
    "      for metric in metrics:\n",
    "        args = metric.split('_')\n",
    "\n",
    "        if len(args) is 2:\n",
    "          m, k = args\n",
    "          k = dict(k=int(k))\n",
    "        elif len(args) is 1:\n",
    "          m, k = args[0], dict()\n",
    "        else:\n",
    "          raise Exception('Invalid string metric.')\n",
    "\n",
    "        results[metric]=getattr(self, m)(**k)\n",
    "\n",
    "      return results\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Supervised metrics (Table 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def MI(source, z, random_state=SEED):\n",
    "    # Thin wrapper for compatibility with pearonr and spearmanr\n",
    "    z = z.reshape((-1, 1))\n",
    "    return mutual_info_regression(z, source,\n",
    "            discrete_features=False, random_state=random_state)[0], None\n",
    "\n",
    "\n",
    "\n",
    "def cuts(source_ref, source_corr, z, corr, **kwargs):\n",
    "    # Partition data over equal-width intervals of source_ref, then apply\n",
    "    # similarity measure corr to source_corr and z over said partitions.\n",
    "    # Return the average\n",
    "    n = 10\n",
    "    slices = np.linspace(source_ref.min(), source_ref.max(), endpoint=True, num=n + 1)\n",
    "    slices[-1] += .1  # Increase last boundary to include last point\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        mask = np.logical_and(source_ref >= slices[i],\n",
    "                              source_ref < slices[i + 1])\n",
    "        source_corr_m, z_m = source_corr[mask], z[mask]\n",
    "        c, _ = corr(source_corr_m, z_m, **kwargs)\n",
    "        total += abs(c)\n",
    "\n",
    "    return total / n\n",
    "\n",
    "\n",
    "def slice_correlator(source_1, source_2, z_1, z_2, corr, name, **kwargs):\n",
    "    # Check with optimal corr if flipping the axes is required\n",
    "    flip_required = optimal_corr(source_1, source_2, z_1, z_2,\n",
    "                                 corr, name, check_flip=True, **kwargs)\n",
    "    if flip_required:\n",
    "        z_temp = z_2\n",
    "        z_2 = z_1\n",
    "        z_1 = z_temp\n",
    "\n",
    "    key_1 = f'{name}_slice_source_1'\n",
    "    key_2 = f'{name}_slice_source_2'\n",
    "\n",
    "    # Compute similariy measure over 20 sections (10 sections orthogonal to\n",
    "    # each axes)\n",
    "    return {\n",
    "        key_1: cuts(source_2, source_1, z_1, corr, **kwargs),\n",
    "        key_2: cuts(source_1, source_2, z_2, corr, **kwargs),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def optimal_corr(source_1, source_2, z_1, z_2, corr, name, check_flip=False, **kwargs):\n",
    "    # Correlate a 2D embedding with two source signals with optimal matching and\n",
    "    # sign flipping\n",
    "\n",
    "    # First candidate\n",
    "    a_1, _ = corr(source_1, z_1, **kwargs)\n",
    "    a_2, _ = corr(source_2, z_2, **kwargs)\n",
    "    a_1 = abs(a_1)\n",
    "    a_2 = abs(a_2)\n",
    "    s_1 = a_1 + a_2\n",
    "\n",
    "    # Second candidate\n",
    "    b_1, _ = corr(source_1, z_2, **kwargs)\n",
    "    b_2, _ = corr(source_2, z_1, **kwargs)\n",
    "    b_1 = abs(b_1)\n",
    "    b_2 = abs(b_2)\n",
    "    s_2 = b_1 + b_2\n",
    "\n",
    "    # Return matching that maximizes correlations over both axes\n",
    "    key_1 = f'{name}_source_1'\n",
    "    key_2 = f'{name}_source_2'\n",
    "\n",
    "    if check_flip:\n",
    "        # Simply return boolean indicating if flipping is required. Used by\n",
    "        # slice_correlator\n",
    "        if s_1 > s_2:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    if s_1 > s_2:\n",
    "        return {key_1: a_1, key_2: a_2}\n",
    "    else:\n",
    "        return {key_1: b_1, key_2: b_2}\n",
    "\n",
    "\n",
    "def optimal_corr_ICA(source_1, source_2, z_1, z_2, corr, name, random_state=SEED, **kwargs):\n",
    "    # Optimal correlation with FastICA preprocessing\n",
    "    z = np.vstack((z_1, z_2)).T\n",
    "    z_transformed = FastICA(random_state=random_state).fit_transform(z)\n",
    "\n",
    "    return optimal_corr(source_1, source_2, *z_transformed.T, corr, name + '_ICA', **kwargs )\n",
    "\n",
    "\n",
    "def supervised_metrics(source_1, source_2, z_1, z_2, random_state=SEED):\n",
    "    # Return three variants of Pearson, Spearman and MI, as well as Mantel\n",
    "    metrics = dict()\n",
    "\n",
    "    for f, name in ((pearsonr, 'pearson'), (spearmanr, 'spearman'), (MI, 'mutual_information')):\n",
    "        args = {}\n",
    "\n",
    "        corr = optimal_corr(source_1, source_2, z_1, z_2, f, name, **args)\n",
    "        corr_ICA = optimal_corr_ICA(source_1, source_2, z_1, z_2, f, name, **args)\n",
    "        corr_slice = slice_correlator(source_1, source_2, z_1, z_2, f, name, **args)\n",
    "        metrics.update({**corr, **corr_ICA, **corr_slice})\n",
    "\n",
    "    # Mantel\n",
    "    source = np.vstack((source_1, source_2)).T\n",
    "    z = np.vstack((z_1, z_2)).T\n",
    "    source_dist = pdist(source, metric='euclidean')\n",
    "    z_dist = pdist(z, metric='euclidean')\n",
    "    mant, _, _ = mantel(source_dist, z_dist, permutations=100)\n",
    "\n",
    "    return {**metrics, 'mantel': mant}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment parameters\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopoAE is not defined. If you wish to use them, please refer to the Topological autoencoders subsection.\n"
     ]
    }
   ],
   "source": [
    "# Build dict with various model parameters\n",
    "\n",
    "# Neighborhood parameters of manifold learners\n",
    "PHATE_knn = dict(\n",
    "    Faces=dict(knn=5),\n",
    "    RotatedDigits=dict(knn=5),\n",
    "    ribbons=dict(knn=20),\n",
    ")\n",
    "\n",
    "UMAP_n_neighbors = dict(\n",
    "    Faces=dict(n_neighbors=15),\n",
    "    RotatedDigits=dict(n_neighbors=15),\n",
    "    ribbons=dict(n_neighbors=20),\n",
    ")\n",
    "\n",
    "TSNE_perp = dict(\n",
    "    Faces=dict(perplexity=10),\n",
    "    RotatedDigits=dict(perplexity=10),\n",
    "    ribbons=dict(perplexity=30),\n",
    ")\n",
    "\n",
    "# t parameter for PHATE\n",
    "t = 'auto'\n",
    "\n",
    "\n",
    "ds_names = dataset_constructors.keys()\n",
    "\n",
    "\n",
    "# Input size. Should be passed to AE and GRAE to adjust\n",
    "# input and output layers accordingly\n",
    "size_dict = dict(\n",
    "              RotatedDigits=dict(input_size=784),\n",
    "              ribbons=dict(input_size=3),\n",
    "              Faces=dict(input_size=4096),\n",
    "            )\n",
    "\n",
    "# Build dict with both input size and knn parameter\n",
    "PHATE_knn_size = copy.deepcopy(PHATE_knn)\n",
    "\n",
    "for key, item in PHATE_knn_size.items():\n",
    "  item.update(size_dict[key])\n",
    "\n",
    "UMAP_n_neighbors_size = copy.deepcopy(UMAP_n_neighbors)\n",
    "\n",
    "for key, item in UMAP_n_neighbors_size.items():\n",
    "  item.update(size_dict[key])\n",
    "\n",
    "TSNE_perp_size = copy.deepcopy(TSNE_perp)\n",
    "\n",
    "for key, item in TSNE_perp_size.items():\n",
    "  item.update(size_dict[key])\n",
    "\n",
    "\n",
    "# As placeholder when no parameters are needed for dataset specific inits\n",
    "empty_dict = dict(zip(ds_names, [{} for _ in ds_names]))\n",
    "\n",
    "\n",
    "# Parameter variable\n",
    "params = {\n",
    "    'Umap': dict( # Vanilla Umap with no transforms, use Umap_t instead\n",
    "        constructor=umap.UMAP,\n",
    "        phate_cache=False,\n",
    "        numpy=True,\n",
    "        train_only=True,\n",
    "        init_default=dict(), # Parameters to be used for all model inits\n",
    "        init_dataset=empty_dict, # Dataset specific model inits\n",
    "        FIT_DEFAULT={}, # Parameters to be used for all model fits\n",
    "        fit_dataset=empty_dict # Dataset specific model fits\n",
    "        ),\n",
    "\n",
    "      'Umap_t': dict(\n",
    "        constructor=umap.UMAP,\n",
    "        phate_cache=False,\n",
    "        numpy=True,\n",
    "        train_only=False,\n",
    "        init_default=dict(),\n",
    "        init_dataset=UMAP_n_neighbors,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "        ),\n",
    "\n",
    "\n",
    "      'PHATE': dict( # vanilla PHATE\n",
    "        constructor=phate.PHATE,\n",
    "        phate_cache=True,\n",
    "        numpy=True,\n",
    "        train_only=True,\n",
    "        init_default=dict(verbose=0, n_jobs=-1, t=t),\n",
    "        init_dataset=PHATE_knn,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "        ),\n",
    "\n",
    "      'TSNE': dict(\n",
    "        constructor=TSNE,\n",
    "        phate_cache=False,\n",
    "        numpy=True,\n",
    "        train_only=True,\n",
    "        init_default=dict(verbose=0, n_jobs=-1),\n",
    "        init_dataset=empty_dict,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "        ),\n",
    "\n",
    "\n",
    "      'GRAE': dict( # This is GRAE.\n",
    "        constructor=GRAE,\n",
    "        phate_cache=True,\n",
    "        numpy=False,\n",
    "        train_only=False,\n",
    "        init_default=dict(lam=1, t=t),\n",
    "        init_dataset=PHATE_knn_size,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "        ),\n",
    "      'GRAE_UMAP': dict(\n",
    "        constructor=GRAE_UMAP,\n",
    "        phate_cache=False,\n",
    "        numpy=False,\n",
    "        train_only=False,\n",
    "        init_default=dict(lam=1),\n",
    "        init_dataset=UMAP_n_neighbors_size,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "      ),\n",
    "      'GRAE_TSNE': dict(\n",
    "        constructor=GRAE_TSNE,\n",
    "        phate_cache=False,\n",
    "        numpy=False,\n",
    "        train_only=False,\n",
    "        init_default=dict(lam=1),\n",
    "        init_dataset=TSNE_perp_size,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "      ),\n",
    "      'AE': dict(\n",
    "        constructor=AE,\n",
    "        phate_cache=False,\n",
    "        numpy=False,\n",
    "        train_only=False,\n",
    "        init_default={},\n",
    "        init_dataset=size_dict,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "        ),\n",
    "}\n",
    "\n",
    "tae = False\n",
    "\n",
    "try:\n",
    "  params.update({\n",
    "      'TopoAE': dict(\n",
    "        constructor=TopoAE,\n",
    "        phate_cache=False,\n",
    "        numpy=False,\n",
    "        train_only=False,\n",
    "        init_default={},\n",
    "        init_dataset=size_dict,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "      )\n",
    "  })\n",
    "  tae = True\n",
    "except Exception:\n",
    "  print('TopoAE is not defined. If you wish to use them, please refer to the Topological autoencoders subsection.')\n",
    "\n",
    "\n",
    "# Add variants of PHATE-Net, UMAP-Net and TSNE-Net\n",
    "for n in (1, 10, 50, 100, 200, 1000, 10000):\n",
    "  params[f'GRAE_{n}'] = copy.deepcopy(params['GRAE'])\n",
    "  params[f'GRAE_{n}']['init_default']['lam'] = n\n",
    "  params[f'GRAE_UMAP_{n}'] = copy.deepcopy(params['GRAE_UMAP'])\n",
    "  params[f'GRAE_UMAP_{n}']['init_default']['lam'] = n\n",
    "  params[f'GRAE_TSNE_{n}'] = copy.deepcopy(params['GRAE_TSNE'])\n",
    "  params[f'GRAE_TSNE_{n}']['init_default']['lam'] = n\n",
    "  if tae:\n",
    "    params[f'TopoAE_{n}'] = copy.deepcopy(params['TopoAE'])\n",
    "    params[f'TopoAE_{n}']['init_default']['lam'] = n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fit loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AE...\n",
      "   On ribbons...\n",
      "       Run 1...\n",
      "   On Faces...\n",
      "       Run 1...\n",
      "Downloading Faces dataset\n",
      "   On RotatedDigits...\n",
      "       Run 1...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n",
      "Training GRAE_10...\n",
      "   On ribbons...\n",
      "       Run 1...\n",
      "   On Faces...\n",
      "       Run 1...\n",
      "   On RotatedDigits...\n",
      "       Run 1...\n",
      "Training GRAE_100...\n",
      "   On ribbons...\n",
      "       Run 1...\n",
      "   On Faces...\n",
      "       Run 1...\n",
      "   On RotatedDigits...\n",
      "       Run 1...\n",
      "Training Umap_t...\n",
      "   On ribbons...\n",
      "       Run 1...\n",
      "   On Faces...\n",
      "       Run 1...\n",
      "   On RotatedDigits...\n",
      "       Run 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%..\\torch\\csrc\\utils\\tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "c:\\users\\morin\\miniconda3\\envs\\ds\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass n_neighbors=750 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "c:\\users\\morin\\miniconda3\\envs\\ds\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass n_neighbors=750 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "# Fit models\n",
    "# All embeddings are saved under the embeddings folder\n",
    "\n",
    "# Remove previous embedding folder if any\n",
    "# !rm -rf 'embeddings'\n",
    "\n",
    "if os.path.exists('embeddings'):\n",
    "    shutil.rmtree('embeddings')\n",
    "\n",
    "os.mkdir('embeddings')\n",
    "\n",
    "# Experiment loop\n",
    "for model in MODELS:\n",
    "  print(f'Training {model}...')\n",
    "\n",
    "  os.mkdir(os.path.join('embeddings', model))\n",
    "\n",
    "  for i, dataset in enumerate(DATASETS):\n",
    "    target = os.path.join('embeddings', model, dataset)\n",
    "\n",
    "    os.mkdir(target)\n",
    "\n",
    "    print(f'   On {dataset}...')\n",
    "    # Training loop\n",
    "    for j in range(RUNS):\n",
    "      print(f'       Run {j + 1}...')\n",
    "\n",
    "      # Fetch and split dataset. Handle numpy input for some models\n",
    "      data_train = dataset_constructors[dataset](split=\"train\",\n",
    "                                                 split_ratio=FIT_RATIO,\n",
    "                                                 seed=SEED)\n",
    "      data_test = dataset_constructors[dataset](split=\"test\",\n",
    "                                                split_ratio=FIT_RATIO,\n",
    "                                                seed=SEED)\n",
    "\n",
    "\n",
    "      data_train_np, y_train = data_train.numpy()\n",
    "      data_test_np, y_test = data_test.numpy()\n",
    "\n",
    "\n",
    "      if params[model]['numpy']:\n",
    "        data_train = data_train_np\n",
    "        data_test = data_test_np\n",
    "\n",
    "\n",
    "\n",
    "      m = params[model]['constructor']( # New Model\n",
    "          **params[model]['init_default'],\n",
    "          **params[model]['init_dataset'][dataset],\n",
    "          random_state=RANDOM_STATES[j])\n",
    "\n",
    "\n",
    "      # Benchmark fit time\n",
    "      fit_start = time.time()\n",
    "\n",
    "      z_train = m.fit_transform(data_train,\n",
    "            **params[model]['FIT_DEFAULT'],\n",
    "            **params[model]['fit_dataset'][dataset])\n",
    "\n",
    "      fit_stop = time.time()\n",
    "\n",
    "      fit_time = fit_stop - fit_start\n",
    "\n",
    "\n",
    "      if not params[model]['train_only']:\n",
    "        # Benchmark transform time if required\n",
    "        transform_start = time.time()\n",
    "        z_test = m.transform(data_test)\n",
    "        transform_stop = time.time()\n",
    "\n",
    "        transform_time = transform_stop - transform_start\n",
    "\n",
    "\n",
    "\n",
    "      if params[model]['train_only']:\n",
    "         # T-SNE and PHATE do not have inverse transforms\n",
    "        inv_train, inv_test, rec_train, rec_test = None, None, None, None\n",
    "      else:\n",
    "        inv_train = m.inverse_transform(z_train)\n",
    "        inv_test = m.inverse_transform(z_test)\n",
    "\n",
    "        rec_train = mean_squared_error(data_train_np, inv_train)\n",
    "        rec_test = mean_squared_error(data_test_np, inv_test)\n",
    "\n",
    "\n",
    "      # Save embeddings\n",
    "      if params[model]['train_only']:\n",
    "        obj = dict(z_train=z_train, z_test=None,\n",
    "                   rec_train=None, rec_test=None,\n",
    "                   fit_time=fit_time, transform_time=None,\n",
    "                   dataset_seed=SEED, run_seed=RANDOM_STATES[j])\n",
    "      else:\n",
    "        obj = dict(z_train=z_train, z_test=z_test,\n",
    "                   rec_train=rec_train, rec_test=rec_test,\n",
    "                   fit_time=fit_time, transform_time=transform_time,\n",
    "                   dataset_seed=SEED, run_seed=RANDOM_STATES[j])\n",
    "\n",
    "\n",
    "      save_dict(obj, os.path.join(target, f'run_{j + 1}.pkl'))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-15-31b1c7c67b22>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[0msplits\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'_'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[0mbase\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlam\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msplits\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msplits\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m     \u001B[0mmodel_name\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mm\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbase_name\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbase\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34mf'{lam})'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Prettier names for models and datasets\n",
    "\n",
    "model_name = dict(Umap_t= 'UMAP',\n",
    "                  Umap='UMAP',\n",
    "                  diffusion_net='Diffusion Nets',\n",
    "                  PHATE='PHATE',\n",
    "                  TSNE='t-SNE',\n",
    "                  AE='Autoencoder')\n",
    "\n",
    "base_name = dict(GRAE='GRAE (', GRAE_TSNE='GRAE t-SNE (',\n",
    "                 GRAE_UMAP='GRAE UMAP (', TopoAE='TAE (')\n",
    "\n",
    "for m in MODELS:\n",
    "  if m not in model_name:\n",
    "    splits = m.split('_')\n",
    "    base, lam = splits[:-1], splits[-1]\n",
    "    model_name[m] = base_name[base] + f'{lam})'\n",
    "\n",
    "\n",
    "\n",
    "ds_name = dict(ribbons='Swiss Roll',\n",
    "               Faces='Faces',\n",
    "               RotatedDigits='Rotated Digits')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot embeddings\n",
    "\n",
    "PLOT_RUN = 1\n",
    "\n",
    "\n",
    "titles = [model_name[m] for m in MODELS]\n",
    "n_d = len(DATASETS)\n",
    "n_m = len(MODELS)\n",
    "fig, ax = plt.subplots(n_d, n_m, figsize=(n_m * 3.5, n_d * 3.5))\n",
    "\n",
    "for j, model in enumerate(MODELS):\n",
    "  for i, dataset in enumerate(DATASETS):\n",
    "    file_path = os.path.join('embeddings', model, dataset, f'run_{PLOT_RUN}.pkl')\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "      # Retrieve datasets for coloring\n",
    "      data = load_dict(file_path)\n",
    "      X_train = dataset_constructors[dataset](split='train',\n",
    "                                              seed=data['dataset_seed'])\n",
    "      X_test = dataset_constructors[dataset](split='test',\n",
    "                                             seed=data['dataset_seed'])\n",
    "      _, y_train = X_train.numpy()\n",
    "      _, y_test = X_test.numpy()\n",
    "      z_train, z_test = data['z_train'], data['z_test']\n",
    "    else:\n",
    "      # Filler if plot is not found\n",
    "      z_train, z_test = np.array([[0, 0]]), np.array([[0, 0]])\n",
    "      y_train = np.array([1])\n",
    "      y_test = np.array([1])\n",
    "\n",
    "\n",
    "    if n_d == 1:\n",
    "      ax_i = ax[j]\n",
    "    elif n_m == 1:\n",
    "      ax_i = ax[i]\n",
    "    else:\n",
    "      ax_i = ax[i, j]\n",
    "\n",
    "    l = ax_i.scatter(*z_train.T, s = 1.5,  alpha=.2, color='grey')\n",
    "\n",
    "    ax_i.scatter(*z_test.T, c = y_test, s = 15, cmap='jet')\n",
    "\n",
    "\n",
    "\n",
    "    if i is 0:\n",
    "      ax_i.set_title(f'{titles[j]}', fontsize=20, color='black')\n",
    "    ax_i.set_xticks([])\n",
    "    ax_i.set_yticks([])\n",
    "\n",
    "\n",
    "plt.savefig('plot.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Score embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Two groups of metrics\n",
    "UNSUPERVISED = True\n",
    "SUPERVISED = True\n",
    "\n",
    "rootdir = 'embeddings'\n",
    "\n",
    "# Datasets on which supervised metrics can be computed\n",
    "DATASETS_SUP = ['ribbons', 'Faces']\n",
    "\n",
    "\n",
    "# Define required metrics\n",
    "# Supervised metrics\n",
    "SUP_METRICS = ['mantel',\n",
    "               'pearson_source_1', 'pearson_source_2',\n",
    "               'pearson_ICA_source_1','pearson_ICA_source_2',\n",
    "               'pearson_slice_source_1', 'pearson_slice_source_2',\n",
    "               'spearman_source_1', 'spearman_source_2',\n",
    "               'spearman_ICA_source_1', 'spearman_ICA_source_2',\n",
    "               'spearman_slice_source_1', 'spearman_slice_source_2',\n",
    "               'mutual_information_source_1', 'mutual_information_source_2',\n",
    "               'mutual_information_ICA_source_1', 'mutual_information_ICA_source_2',\n",
    "               'mutual_information_slice_source_1', 'mutual_information_slice_source_2'\n",
    "]\n",
    "\n",
    "\n",
    "# Usupervised Metrics\n",
    "metrics_uns = list()\n",
    "\n",
    "for m in ('continuity', 'trustworthiness', 'mrre'):\n",
    "  for k in (5, 10):\n",
    "    metrics_uns.append(f'{m}_{k}')\n",
    "\n",
    "\n",
    "# File to save data\n",
    "unsupervised_name = 'unsupervised_raw.csv'\n",
    "supervised_name = 'supervised_raw.csv'\n",
    "\n",
    "\n",
    "# Loogers for results\n",
    "book_uns = Book(models=MODELS,\n",
    "                datasets=DATASETS,\n",
    "                metrics=metrics_uns + ['reconstruction'])\n",
    "\n",
    "book_sup = Book(models=MODELS,\n",
    "                datasets=DATASETS,\n",
    "                metrics=SUP_METRICS)\n",
    "\n",
    "# Iterate over all embeddings\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "\n",
    "    for file in files:\n",
    "        _, model, dataset = os.path.normpath(subdir).split(os.sep)\n",
    "\n",
    "        if dataset not in DATASETS:\n",
    "            continue\n",
    "        if model not in MODELS:\n",
    "            continue\n",
    "\n",
    "        filepath = subdir + os.sep + file\n",
    "        print(f'Scoring {filepath}...')\n",
    "        data = load_dict(filepath)\n",
    "\n",
    "        dataset_seed = data['dataset_seed']\n",
    "        run_seed = data['run_seed']\n",
    "\n",
    "\n",
    "        # Only compute on test split for the paper\n",
    "        X_test = dataset_constructors[dataset](split='test', seed=dataset_seed)\n",
    "        x_test, y_test = X_test.numpy()\n",
    "        z_test = data['z_test']\n",
    "\n",
    "\n",
    "        if UNSUPERVISED:\n",
    "            scorer_test = MeasureCalculator(x_test, z_test, None)\n",
    "            test_metrics = scorer_test.get_metrics(metrics_uns)\n",
    "            test_metrics.update(dict(reconstruction=data['rec_test']))\n",
    "\n",
    "            book_uns.add_entry(model=model, dataset=dataset, run=run_seed,\n",
    "                               split='test', **test_metrics)\n",
    "\n",
    "        if SUPERVISED and dataset in DATASETS_SUP:\n",
    "            y_1_test, y_2_test = X_test.get_source()\n",
    "\n",
    "            metrics_sup = supervised_metrics(y_1_test, y_2_test,\n",
    "                                             *z_test.T)\n",
    "\n",
    "            book_sup.add_entry(model=model, dataset=dataset,\n",
    "                               run=run_seed, split='test', **metrics_sup)\n",
    "\n",
    "if UNSUPERVISED:\n",
    "    df = book_uns.get_df()\n",
    "    df.to_csv(unsupervised_name)\n",
    "\n",
    "if SUPERVISED:\n",
    "    df = book_sup.get_df()\n",
    "    df.to_csv(supervised_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define beter names for dataframe columns in next cell\n",
    "col_map = dict(dataset='Dataset', model='Model',\n",
    "               continuity='Cont', trustworthiness='Trust', mrre='MRRE',\n",
    "               reconstruction='Reconstruction',\n",
    "               corr_source='Correlation',\n",
    "               corr_source_ICA='Correlation ICA',\n",
    "               pearson='Pearson',\n",
    "               spearman='Spearman',\n",
    "               mutual_information='MI',\n",
    "               pearson_ICA='Pearson ICA',\n",
    "               spearman_ICA='Spearman ICA',\n",
    "               mutual_information_ICA='MI ICA',\n",
    "               pearson_slice='Pearson Section',\n",
    "               spearman_slice='Spearman Section',\n",
    "               mutual_information_slice='MI Section',\n",
    "               mantel='Mantel',\n",
    "               corr_source_patch='Patch Correlation',\n",
    "               corr_source_ICA_patch='Patch Correlation ICA',\n",
    "               mantel_patch='Patch Mantel'\n",
    ")\n",
    "\n",
    "for i in (5, 10, 20):\n",
    "    for s in ('continuity', 'trustworthiness', 'mrre'):\n",
    "        base = col_map[s]\n",
    "        col_map[f'{s}_{i}'] = f'{base} ({i})'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show metrics function\n",
    "def show_metrics(supervised):\n",
    "  df = pd.read_csv('supervised_raw.csv'\n",
    "                   if supervised else 'unsupervised_raw.csv', index_col=0)\n",
    "\n",
    "  # Keep only test data from MODELS and DATASETS\n",
    "  df = df[df['model'].isin(MODELS)]\n",
    "  df = df[df['dataset'].isin(DATASETS)]\n",
    "  df = df[df['split'] == 'test']\n",
    "\n",
    "  df = df.drop(columns=['split', 'run'])\n",
    "\n",
    "  metrics_datasets = copy.deepcopy(DATASETS)\n",
    "\n",
    "  if supervised:\n",
    "    # Do not include Rotated Digits, as they don't have a clear ground truth\n",
    "    metrics_datasets.remove('RotatedDigits')\n",
    "    # Average pearson, spearman and MI over source 1 and source 2\n",
    "    df = df[df.dataset != 'RotatedDigits']\n",
    "    for base in ('pearson', 'spearman', 'mutual_information'):\n",
    "        df[f'{base}'] = (df[f'{base}_source_1'] + df[f'{base}_source_2'])/2\n",
    "        df[f'{base}_ICA'] = (df[f'{base}_ICA_source_1'] + df[f'{base}_ICA_source_2'])/2\n",
    "        df[f'{base}_slice'] = (df[f'{base}_slice_source_1'] + df[f'{base}_slice_source_2'])/2\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    df = df[['dataset', 'model', 'mantel',\n",
    "                 'pearson', 'pearson_ICA', 'pearson_slice',\n",
    "                 'spearman', 'spearman_ICA', 'spearman_slice',\n",
    "                 'mutual_information', 'mutual_information_ICA', 'mutual_information_slice',\n",
    "                 ]]\n",
    "\n",
    "\n",
    "  # Provide order for models and datasets if needed\n",
    "  df['model'] = pd.Categorical(df['model'], MODELS)\n",
    "  df['dataset'] = pd.Categorical(df['dataset'], metrics_datasets)\n",
    "\n",
    "  # Prettify names\n",
    "  df['model'] = df['model'].map(model_name)\n",
    "  df['dataset'] = df['dataset'].map(ds_name)\n",
    "\n",
    "  # Prettify column names\n",
    "  df = df.rename(columns=col_map)\n",
    "\n",
    "  # Mean\n",
    "  return df.groupby(['Dataset', 'Model']).mean().round(3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# View unsupervised metrics\n",
    "show_metrics(supervised=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# View supervised metrics\n",
    "show_metrics(supervised=True)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}